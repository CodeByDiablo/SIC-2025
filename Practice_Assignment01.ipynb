{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNW7fEwwQTCE7xIdSzJyVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeByDiablo/SIC-2025/blob/main/Practice_Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI/ML Math Assessment:\n",
        "\n",
        "Student Name: Devesh Panwar\n",
        "\n",
        "Candidate ID: SIC202500060"
      ],
      "metadata": {
        "id": "t-6F1OeczCht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector-Valued & Multivariate Functions:\n",
        "This is a multivariate function because it takes three distinct inputs: speed $v$, distance $d$, and steering angle $\\theta$. It is a vector-valued function because the output is a vector where each dimension represents a different physical behavior: forward velocity, lateral velocity, and a risk factor."
      ],
      "metadata": {
        "id": "hXWwyGFNt03s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical Representation:$$f(v,d,\\theta)=\\begin{bmatrix}v \\cos \\theta \\\\ v \\sin \\theta \\\\ \\frac{v}{d} \\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "ks2TtAc-t-Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Importing the numpy Library\n",
        "\n",
        "def vehicle_control(v, d, theta):\n",
        "    # Returns a vector where each dimension is a specific control output\n",
        "    return np.array([\n",
        "        v * np.cos(theta), # forward velocity component\n",
        "        v * np.sin(theta), # lateral velocity component\n",
        "        v / d              # risk factor (higher when obstacle is closer)\n",
        "    ])\n",
        "\n",
        "# Input values provided in the case\n",
        "v, d, theta = 10, 5, np.pi/6\n",
        "output = vehicle_control(v, d, theta)\n",
        "print(\"Vehicle Control Output Vector:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm6dcrabvriV",
        "outputId": "547b9c60-267f-4910-d150-04660a0e3dac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vehicle Control Output Vector: [8.66025404 5.         2.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Jacobian matrix determines how each output changes with respect to each input. In neural networks, gradients are multiplied by Jacobians layer-by-layer during backpropagation. This helps identify exploding gradients (large values) or vanishing gradients (small values)."
      ],
      "metadata": {
        "id": "mPvr2g0pvutT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobian Computation:\n",
        "\n",
        "For $f(x,y)=\\begin{bmatrix}e^{x+y}\\\\ xy^{2}\\end{bmatrix}$:\n",
        "\n",
        "$$J = \\begin{bmatrix} e^{x+y} & e^{x+y} \\\\ y^2 & 2xy \\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "71ryeYUDv_zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def jacobian(x, y):\n",
        "    # Implementing the Jacobian matrix for the neural network layer\n",
        "    return np.array([\n",
        "        [np.exp(x+y), np.exp(x+y)], # Row 1: Partial derivatives of e^(x+y)\n",
        "        [y**2, 2*x*y]               # Row 2: Partial derivatives of xy^2\n",
        "    ])\n",
        "\n",
        "x, y = 1.0, 2.0\n",
        "print(\"Jacobian Matrix at (1,2):\\n\", jacobian(x, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QqFbjfSwnh5",
        "outputId": "d0fb8a33-5179-489d-cfcf-655f167a0e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobian Matrix at (1,2):\n",
            " [[20.08553692 20.08553692]\n",
            " [ 4.          4.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hessian matrix captures the curvature of the loss surface. A positive definite Hessian (positive eigenvalues) indicates a convex loss function. This is important because convex functions have a single global minimum, ensuring stable and predictable training without being trapped in local minima."
      ],
      "metadata": {
        "id": "q9z17NGFw4ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hessian matrix for L(w1, w2) = w1^2 + 3w2^2 + 2w1w2\n",
        "H = np.array([[2, 2],\n",
        "              [2, 6]])\n",
        "\n",
        "eigenvalues = np.linalg.eigvals(H)\n",
        "print(\"Eigenvalues:\", eigenvalues)\n",
        "\n",
        "# If all eigenvalues > 0, the Hessian is positive definite (Convex)\n",
        "print(\"Is the loss convex?\", np.all(eigenvalues > 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ztsi85fxMbA",
        "outputId": "0f2c972a-8f70-4e77-efae-60ed6968c181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues: [1.17157288 6.82842712]\n",
            "Is the loss convex? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taylor Series - Model Approximation\n",
        "Explanation: Taylor expansion is used in Explainable AI (XAI) to approximate complex model behavior locally. By simplifying a complex model into a linear or quadratic form, it becomes much easier to understand how specific input changes affect the output."
      ],
      "metadata": {
        "id": "tWnRUj9bxjIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second-order Taylor expansion at (0,0):$$f(x,y) \\approx 1 + (x+y) + 0.5(x+y)^2$$"
      ],
      "metadata": {
        "id": "-2wVHyiNx0x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def taylor_approx(x, y):\n",
        "    # Quadratic approximation of the model behavior around (0,0)\n",
        "    return 1 + (x+y) + 0.5 * (x+y)**2\n",
        "\n",
        "x, y = 0.1, 0.2\n",
        "print(\"Actual Model Output:\", np.exp(x+y))\n",
        "print(\"Taylor Approximation Output:\", taylor_approx(x, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj6qnMaRx4Wo",
        "outputId": "a1bbadd7-5023-4d66-c23c-61b020fcc683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Model Output: 1.3498588075760032\n",
            "Taylor Approximation Output: 1.345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraud Detection Optimization:\n",
        "\n",
        "Analyzing the Hessian at critical points allows us to classify the point's nature.\n",
        "\n",
        "Positive Definite: Indicates a local minimum.\n",
        "\n",
        "Negative Definite: Indicates a local maximum.\n",
        "\n",
        "Indefinite (Mixed Eigenvalues): Indicates a saddle point.\n",
        "\n",
        "Critical Points Analysis for $J(x,y)=x^{4}+y^{4}-4xy$:\n",
        "\n",
        "Find Gradients: $\\nabla J = [4x^3 - 4y, 4y^3 - 4x] = [0, 0]$.\n",
        "\n",
        "Critical Points: $(0,0), (1,1), (-1,-1)$.\n",
        "\n",
        "Classification: This is vital for avoiding saddle points during training, which can stall the optimization process."
      ],
      "metadata": {
        "id": "2Ty9in1t2mwc"
      }
    }
  ]
}